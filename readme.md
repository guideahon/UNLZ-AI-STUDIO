![Logo Institucional](https://github.com/JonatanBogadoUNLZ/PPS-Jonatan-Bogado/blob/9952aac097aca83a1aadfc26679fc7ec57369d82/LOGO%20AZUL%20HORIZONTAL%20-%20fondo%20transparente.png)

# Universidad Nacional de Lomas de Zamora â€“ Facultad de IngenierÃ­a
## UNLZ-AI-STUDIO
---
## Objetivo del repositorio

Uso educativo y de laboratorio
Que una universidad (como la UNLZ) pueda tener un servidor local donde docentes y estudiantes:

Hagan TPs y prototipos con LLMs/VLMs.

Prueben chatbots, asistentes de programaciÃ³n, anÃ¡lisis de imÃ¡genes, STT y TTS.

Programen contra una sola API sin importar quÃ© modelo o backend hay por detrÃ¡s.

ðŸ› ï¸ Simplificar el self-hosting en PCs reales

Detecta CPU/RAM/GPU y elige automÃ¡ticamente un perfil (alto, medio, baja, cpu).

Configura parÃ¡metros de llama.cpp (ctx, n-gpu-layers, batch, etc.) segÃºn el equipo.

Evita que llama-server y lmdeploy peleen por la VRAM (auto-switch).

ðŸ§© Ofrecer una puerta de entrada clara para hobbistas

GuÃ­a paso a paso para instalar modelos, dependencias y TTS en Windows.

Endpoints listos para consumir desde Python, PowerShell, ESP32, etc.

Ejemplos concretos de uso: textoâ†’texto, imagen+texto, audioâ†’textoâ†’voz.

ðŸ–¥ï¸ Dar una interfaz â€œhumanaâ€ para operar el servidor

GUI en Tkinter (estilo IngenierÃ­a) y **Web UI (Next.js)** para:

Ver hardware y perfil activo.

Levantar/apagar servidores.

Activar/desactivar endpoints.

Ajustar presets y modelos personalizados.

---

## ðŸš€ Arquitectura Modular

UNLZ-AI-STUDIO cuenta con un sistema de **MÃ³dulos Pluggables** que permite extender la funcionalidad de la plataforma. Los mÃ³dulos pueden instalarse desde la **GUI** (o Web UI) y se encuentran en la carpeta `modules/`.

### MÃ³dulos Incluidos

#### 1. Gaussian Splatting (VisualizaciÃ³n 3D)
- **Ruta**: `modules/gaussian/`
- **Funcionalidad**: Permite crear escenas 3D a partir de imÃ¡genes utilizando **SharpSplat**.
- **Interfaz**: Visor 3D interactivo integrado en la aplicaciÃ³n.

#### 2. LLM Frontend (Chat & Manager)
- **Ruta**: `modules/llm_frontend/`
- **Funcionalidad**:
    - **Chat**: Interfaz grÃ¡fica para conversar con los modelos locales servidos por `gateway.py`.
    - **Gestor de Modelos**: Escanea tu carpeta de modelos (`C:\models`) y permite cambiar el modelo activo con un clic.
    - **Descargas**: Descarga modelos GGUF directamente desde Hugging Face usando RepoID.

#### 3. Inclu-IA (Subtitulado en Tiempo Real)
- **Ruta**: `modules/inclu_ia/`
- **DescripciÃ³n**: Sistema de accesibilidad para aulas.
- **Funcionamiento**: Convierte tu PC en un servidor de subtÃ­tulos. Captura audio del micrÃ³fono, lo transcribe con IA (Faster-Whisper) y lo distribuye vÃ­a Web (Wi-Fi local) a los dispositivos de los alumnos.
- **Origen**: AdaptaciÃ³n del proyecto homÃ³nimo para Raspberry Pi.

---

## ðŸš€ API Gateway

El `gateway.py` sigue siendo el nÃºcleo que gestiona los procesos pesados:
- **/llm** â€“ textoâ†”texto con **llama.cpp**
- **/clm** â€“ textoâ†”texto con **HF Transformers**
- **/vlm** â€“ imagen+prompt con **LMDeploy**
- **/alm** â€“ audioâ†’textoâ†’LLMâ†’voz
- **/slm** â€“ streaming audio/texto

---

## Ajustes automaticos de hardware

Desde `gateway.py` ahora se detectan CPU, RAM y GPU al iniciar, y se elige el preset mas apropiado para `llama-server`. Los perfiles principales son:
- `ultra` y `alto`: equipos con >=16 GB de VRAM y 64+ GB de RAM, usan Qwen3-Coder-30B con mas capas en GPU y contexto amplio.
- `balanceado` y `medio`: GPUs de 8 a 12 GB con 32-48 GB de RAM, priorizan Qwen3-Coder-14B y ajustan `--n-gpu-layers` para evitar OOM.
- `baja`: pensado para RTX 2060/3050 (6 GB) con 24-32 GB de RAM.
- `cpu`: modo de emergencia cuando no hay GPU o no hay espacio de VRAM; usa **Qwen2.5-Coder-7B**.

---

## ðŸ“¦ InstalaciÃ³n

> **PowerShell:** el continuador de lÃ­nea es el **backtick** `` ` `` y debe ir como **Ãºltimo carÃ¡cter** (sin espacios despuÃ©s).

### 1) Modelos

```powershell
# LLM (GGUF) â€“ Qwen3-Coder-30B-A3B-Instruct (Q5_K_M)
hf download unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF `
  --include "*Q5_K_M*.gguf" `
  --local-dir "C:\models\qwen3-coder-30b"

# VLM (safetensors oficial para LMDeploy) â€“ Qwen2.5-VL-7B-Instruct
hf download Qwen/Qwen2.5-VL-7B-Instruct `
  --local-dir "C:\models\qwen2.5-vl-7b-hf"

# LLM (Perfil Bajo/CPU) - Qwen2.5-Coder-7B-Instruct
hf download Qwen/Qwen2.5-Coder-7B-Instruct-GGUF `
  --include "qwen2.5-coder-7b-instruct-q4_k_m.gguf" `
  --local-dir "C:\models\qwen2.5-coder-7b"

# Piper: modelo + config (ambos necesarios) â€“ voz es_AR/daniela/high
hf download rhasspy/piper-voices `
  --include "es/es_AR/daniela/high/es_AR-daniela-high.onnx*" `
  --local-dir "C:\piper\voices\es_AR\daniela_high"
```

### 2) Dependencias

```powershell
# llama.cpp
winget install llama.cpp

# Python requirements
pip install -U fastapi uvicorn httpx psutil python-multipart faster-whisper
pip install -U lmdeploy
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio
pip install -U huggingface_hub transformers accelerate pillow requests

# GUI & Modules
pip install customtkinter flask flask-socketio SpeechRecognition pyaudio
```

**Rutas esperadas por el gateway:**
- **GGUF 30B:** `C:\models\qwen3-coder-30b\Qwen3-Coder-30B-A3B-Instruct-Q5_K_M.gguf`
- **GGUF 7B:** `C:\models\qwen2.5-coder-7b\qwen2.5-coder-7b-instruct-q4_k_m.gguf`
- **VLM:**  `C:\models\qwen2.5-vl-7b-hf`
- **Piper:** `C:\piper\voices\es_AR\daniela_high\...`

---

## ðŸ§ª Ejemplos de uso

### LLM (Perfil Bajo - Qwen2.5 7B)
```powershell
$body = @{
  model = "qwen2.5-coder-7b"
  messages = @(
    @{ role="system"; content="You are an expert in Finite element analysis." },
    @{ role="user";   content="Explica la diferencia entre analisis lineal y no lineal" }
  )
} | ConvertTo-Json -Depth 5

$web = Invoke-WebRequest -Uri "http://localhost:8000/llm" -Method Post -Body ([Text.Encoding]::UTF8.GetBytes($body)) -ContentType "application/json; charset=utf-8" 
$response = $web.Content | ConvertFrom-Json 
Write-Output $response.choices[0].message.content
```

### Web UI (Nueva MigraciÃ³n)
Para acceder a la nueva interfaz web:
```bash
cd web_ui
npm install
npm run dev
# Abrir http://localhost:3000
```
Si queres, puedo sumar telemetria basica de estado (CPU/RAM/GPU en tiempo real) o un boton de "Abrir docs" en la Home.

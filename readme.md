![Logo Institucional](https://github.com/JonatanBogadoUNLZ/PPS-Jonatan-Bogado/blob/9952aac097aca83a1aadfc26679fc7ec57369d82/LOGO%20AZUL%20HORIZONTAL%20-%20fondo%20transparente.png)

# Universidad Nacional de Lomas de Zamora ‚Äì Facultad de Ingenier√≠a  
## UNLZ-AI-STUDIO

Gu√≠a pr√°ctica de *self-hosting* de LLMs/VLMs por API para universidades y hobbistas.

---

## üöÄ ¬øQu√© expone este proyecto?

- **/llm** ‚Äì texto‚Üîtexto con **llama.cpp** (Qwen3-Coder-30B, GGUF)  
- **/vlm** ‚Äì imagen+prompt con **Qwen2.5-VL-7B** v√≠a **LMDeploy**  
- **/alm** ‚Äì audio‚Üítexto (**STT**) ‚Üí LLM ‚Üí texto‚Üívoz (**TTS**)

‚û°Ô∏è **Auto-switch**: el gateway levanta/para `llama-server` o `lmdeploy` para no pelear VRAM en una sola GPU.

---

## üß© Requisitos PREVIOS

- Windows 10/11 con PowerShell  
- Python 3.10+ (64-bit) https://www.python.org/downloads/
- Conexi√≥n a internet para descargar modelos
- CUDA Toolkit https://developer.nvidia.com/cuda-toolkit

---

## üì¶ Instalaci√≥n

> **PowerShell:** el continuador de l√≠nea es el **backtick** `` ` `` y debe ir como **√∫ltimo car√°cter** (sin espacios despu√©s).

### 1) Modelos

```powershell
# LLM (GGUF) ‚Äì Qwen3-Coder-30B-A3B-Instruct (Q5_K_M)
hf download unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF `
  --include "*Q5_K_M*.gguf" `
  --local-dir "C:\models\qwen3-coder-30b"

# VLM (safetensors oficial para LMDeploy) ‚Äì Qwen2.5-VL-7B-Instruct
hf download Qwen/Qwen2.5-VL-7B-Instruct `
  --local-dir "C:\models\qwen2.5-vl-7b-hf"

# Piper: modelo + config (ambos necesarios) ‚Äì voz es_AR/daniela/high
hf download rhasspy/piper-voices `
  --include "es/es_AR/daniela/high/es_AR-daniela-high.onnx*" `
  --local-dir "C:\piper\voices\es_AR\daniela_high"

> Tip: si prefer√≠s sin backticks, ejecut√° cada `hf download` en **una sola l√≠nea**.

üîä Instalar Piper (TTS) desde GitHub ‚Äî recomendado en Windows

El wrapper de pip install piper-tts puede fallar en Windows. Us√° el binario nativo.

Abr√≠ Releases: https://github.com/rhasspy/piper/releases

Descarg√° el asset piper_windows_amd64.zip de la versi√≥n m√°s reciente.

Cre√° la carpeta y descomprim√≠ directo ah√≠:

New-Item -ItemType Directory -Force -Path "C:\piper" | Out-Null
$zip = "$env:TEMP\piper_windows_amd64.zip"
Invoke-WebRequest "https://github.com/rhasspy/piper/releases/download/2023.11.14-2/piper_windows_amd64.zip" -OutFile $zip
Expand-Archive $zip -DestinationPath "C:\piper" -Force
Remove-Item $zip

```

### 2) Dependencias

```powershell
# llama.cpp (Vulkan por winget; si luego quer√©s CUDA, baj√° el zip cublas desde Releases)
winget install llama.cpp

# Backend y gateway
pip install -U fastapi uvicorn httpx psutil python-multipart faster-whisper

# VLM (LMDeploy)
pip install -U lmdeploy

# PyTorch CUDA (CUDA 12.1 wheels; ajust√° si us√°s otra versi√≥n)
pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio

# Hugging Face (incluye el comando 'hf')
pip install -U huggingface_hub

# Necesario para llamar a qwen_vl
pip install -U transformers accelerate pillow requests qwen_vl_utils

#TurboMind offline mode
lmdeploy convert hf Qwen/Qwen2.5-VL-7B-Instruct `
  --dst-path "C:\models\qwen2.5-vl-7b-tm"
```

**Rutas esperadas por el gateway:**
- **GGUF:** `C:\models\qwen3-coder-30b\Qwen3-Coder-30B-A3B-Instruct.Q5_K_M.gguf`  
- **VLM:**  `C:\models\qwen2.5-vl-7b-hf`  
- **Piper:** `C:\piper\voices\es_AR\daniela_high\...`

---

## ‚ñ∂Ô∏è Ejecutar el gateway

```powershell
python .\gateway.py
# (o)
uvicorn gateway:app --host 0.0.0.0 --port 8000
```

- **/llm** ‚Üí proxyea a **llama-server** (si no est√°, lo levanta y apaga `lmdeploy`)  
- **/vlm** ‚Üí proxyea a **LMDeploy** (si no est√°, lo levanta y apaga `llama-server`)  
- **/alm** ‚Üí corre **STT** (CPU por defecto), luego **/llm**, y **TTS** con Piper (devuelve WAV en **data:base64**)

---

## üß™ Ejemplos (PowerShell)

### LLM (texto)
```powershell
# 1) Health (opcional)
Invoke-RestMethod "http://localhost:8000/health"

# 2) Body JSON
$body = @{
  model = "qwen3-coder-30b"
  messages = @(
    @{ role="system"; content="You are a helpful coding assistant." },
    @{ role="user";   content="Explic√° PID con pseudoc√≥digo." }
  )
} | ConvertTo-Json -Depth 5

# 3) Enviar (UTF-8) y ver la respuesta completa (raw JSON)
$raw = Invoke-WebRequest -Uri "http://localhost:8000/llm" `
  -Method Post `
  -ContentType "application/json; charset=utf-8" `
  -Body ([Text.Encoding]::UTF8.GetBytes($body)) `
| Select-Object -ExpandProperty Content
$resp = $raw | ConvertFrom-Json
$msg  = $resp.choices[0].message
if ($msg.content -is [array]) {
  ($msg.content | ForEach-Object { $_.text }) -join "`n"
} else {
  $msg.content
}


```

### VLM (visi√≥n, estilo OpenAI)
```powershell
# IMPORTANTE: la imagen debe ser accesible por el servidor (URL p√∫blica)
$imageUrl = "https://upload.wikimedia.org/wikipedia/commons/3/3a/PCB_SMD.jpg"

$body = @{
  model = "qwen2.5-vl-7b"
  messages = @(
    @{
      role    = "user"
      content = @(
        @{ type = "text";      text = "¬øQu√© dice esta placa y qu√© falla ves?" },
        @{ type = "image_url"; image_url = @{ url = $imageUrl } }
      )
    }
  )
} | ConvertTo-Json -Depth 8

# Enviar (UTF-8) y ver raw JSON
$raw = Invoke-WebRequest -Uri "http://localhost:8000/vlm" `
  -Method Post `
  -ContentType "application/json; charset=utf-8" `
  -Body ([Text.Encoding]::UTF8.GetBytes($body)) `
| Select-Object -ExpandProperty Content
$raw

# Extraer texto (string o array de partes)
$resp = $raw | ConvertFrom-Json
$msg  = $resp.choices[0].message
if ($msg.content -is [array]) {
  ($msg.content | ForEach-Object { $_.text }) -join "`n"
} else {
  $msg.content
}

```

### ALM (audio completo) IMPORTANTE, CORRER EL COMANDO CON LA TERMINAL ABIERTA EN LA MISMA RUTA DE gateway.py

```powershell 5

$ErrorActionPreference = "Stop"

# Rutas basadas en la carpeta actual
$here   = (Get-Location).Path
$in     = Join-Path $here 'test.wav'
$outDir = Join-Path $here 'audio-out'
$out    = Join-Path $outDir 'respuesta.wav'

if (-not (Test-Path $in)) { throw "No se encontr√≥ el archivo de entrada: $in" }
New-Item -ItemType Directory -Path $outDir -Force | Out-Null

# ----- Enviar multipart/form-data con .NET HttpClient (compatible con PS 5.1) -----
Add-Type -AssemblyName System.Net.Http

$client = [System.Net.Http.HttpClient]::new()
$mp     = [System.Net.Http.MultipartFormDataContent]::new()

# Parte de archivo
$fs = [System.IO.FileStream]::new($in, [System.IO.FileMode]::Open, [System.IO.FileAccess]::Read)
$sc = [System.Net.Http.StreamContent]::new($fs)
$sc.Headers.ContentType = [System.Net.Http.Headers.MediaTypeHeaderValue]::Parse("audio/wav")
$mp.Add($sc, "file", [System.IO.Path]::GetFileName($in))

# Partes de texto
$mp.Add([System.Net.Http.StringContent]::new("Sos un asistente de mecatr√≥nica que responde de forma clara."), "system_prompt")
$mp.Add([System.Net.Http.StringContent]::new("true"), "tts")
$mp.Add([System.Net.Http.StringContent]::new("es"), "target_lang")

try {
  $resp = $client.PostAsync("http://localhost:8000/alm", $mp).Result
  if (-not $resp.IsSuccessStatusCode) {
    $body = $resp.Content.ReadAsStringAsync().Result
    throw "HTTP $($resp.StatusCode) $($resp.ReasonPhrase) :: $body"
  }
  $raw = $resp.Content.ReadAsStringAsync().Result
}
finally {
  $fs.Dispose()
  $client.Dispose()
}

# ----- Procesar respuesta -----
$jr = $raw | ConvertFrom-Json
$jr.stt_text
$jr.llm_text

# Mostrar error de TTS si vino
if ($jr.tts_error) {
  "TTS error: $($jr.tts_error)"
}

if ($jr.tts_audio) {
  $b64   = ($jr.tts_audio -split ",",2)[-1]
  $bytes = [Convert]::FromBase64String($b64)

  # Evitar escribir WAV vac√≠o: 44 bytes es el m√≠nimo del header RIFF/WAVE
  if ($bytes.Length -ge 44) {
    [IO.File]::WriteAllBytes($out, $bytes)
    "Audio guardado en: $out (bytes: $($bytes.Length))"
  } else {
    "TTS vino vac√≠o (bytes=$($bytes.Length)). No se guard√≥ archivo."
    if ($jr.tts_error) { "Detalle: $($jr.tts_error)" }
  }
} else {
  "No vino audio (tts_audio = null)." + ($(if ($jr.tts_error) { " Detalle: $($jr.tts_error)" } else { "" }))
}



```

```powershell 7
# Rutas (basadas en la carpeta actual)
$here   = (Get-Location).Path
$in     = Join-Path $here 'test.wav'
$outDir = Join-Path $here 'audio-out'
$out    = Join-Path $outDir 'respuesta.wav'

if (-not (Test-Path $in)) {
  Write-Error "No se encontr√≥ el archivo de entrada: $in"
  return
}

# Crear carpeta de salida si no existe
New-Item -ItemType Directory -Path $outDir -Force | Out-Null

# Enviar WAV/MP3/M4A, etc. como multipart/form-data
$resp = Invoke-RestMethod -Uri "http://localhost:8000/alm" -Method Post -Form @{
  file          = Get-Item $in
  system_prompt = "Sos un asistente de mecatr√≥nica que responde de forma clara."
  tts           = "true"
  target_lang   = "es"
}

# Ver STT y respuesta de LLM
$resp.stt_text
$resp.llm_text

# Guardar el WAV devuelto (es data:audio/wav;base64,XXXXX)
if ($resp.tts_audio) {
  $b64   = ($resp.tts_audio -split ",",2)[-1]
  $bytes = [Convert]::FromBase64String($b64)
  [IO.File]::WriteAllBytes($out, $bytes)
  "Audio guardado en: $out"
} else {
  "No vino audio (tts_audio = null)."
}


```

---


## ‚öôÔ∏è Notas t√©cnicas para `gateway.py`

- Para **LMDeploy 0.9.2**:
  - Us√° repo HF posicional + --download-dir con tu carpeta local:
    ```python
    LMDEPLOY_CMD  = [sys.executable, "-m", "lmdeploy"]
    VLM_MODEL_ID  = "Qwen/Qwen2.5-VL-7B-Instruct"
    VLM_CACHE_DIR = r"C:\models\qwen2.5-vl-7b-hf"
    VLM_ARGS = [
      "serve", "api_server", VLM_MODEL_ID,
      "--backend", "pytorch", "--model-format", "hf",
      "--download-dir", VLM_CACHE_DIR,
      "--server-port", "9090",
    ]
    ```
  - Alternativa 100% offline: convertir a TurboMind y servir la carpeta convertida.

- Para **llama.cpp**:
  ```python
  LLAMA_ARGS = [
      "-m", LLAMA_MODEL,
      "--host", LLAMA_HOST, "--port", str(LLAMA_PORT),
      "--ctx-size", "8192",
      "--n-gpu-layers", "35",
      "-t", "12"
      # "--flash-attn"  # solo si us√°s CUDA/cuBLAS (no Vulkan)
    ]
  ```
  Si tu versi√≥n de llama da error con `--n-gpu-layers`, prob√° `--ngl`.

---

## üõ°Ô∏è Consejos r√°pidos

- Si us√°s **llama Vulkan** (winget), quit√° `--flash-attn` en `LLAMA_ARGS`.  
- ¬øOOM? Baj√° `--n-gpu-layers` o `--ctx-size`, o cuantiz√° a **Q4_K_M**.  
- **STT en GPU**: `USE_CUDA_FOR_STT=True` (puede ir m√°s lento si coincide con el LLM).  
- Para exponer en LAN, abr√≠ el **puerto 8000** en el firewall de Windows:
  ```powershell
  netsh advfirewall firewall add rule name="UNLZ-AI-STUDIO 8000" dir=in action=allow protocol=TCP localport=8000
  ```

---

## üìÇ Estructura del repositorio

- C√≥digo fuente del **gateway** y utilitarios  
- Documentaci√≥n t√©cnica y gu√≠as de uso  
- Ejemplos de clientes (Python/JS) para consumir la API

---

## üìú Licencia y uso

Los proyectos aqu√≠ incluidos pueden tener diferentes licencias, seg√∫n lo definido por sus autores.  
Antes de usar o modificar, revis√° el archivo **LICENSE** correspondiente.

---

## üåê Sitios

- **P√°gina principal:** https://unlzfi.github.io/  
- **Repositorios:** https://github.com/orgs/UNLZFI/repositories  
- **Facultad de Ingenier√≠a ‚Äì UNLZ:** https://ingenieria.unlz.edu.ar/

---

## üì¨ Contacto

**Facultad de Ingenier√≠a ‚Äì UNLZ**  
Carrera de Ingenier√≠a en Mecatr√≥nica
